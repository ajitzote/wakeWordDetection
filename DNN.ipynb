{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovuopBifxSe0",
        "outputId": "6be04fd7-5ee8-4f69-abaf-c236da8fd206"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-f05342cf58f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlfilter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import os \n",
        "from scipy.signal import lfilter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DAaGfMoxSe4"
      },
      "outputs": [],
      "source": [
        "def pre_emphasize(input):\n",
        "    \"\"\" INPUTS: input: input speech signal array, alpha = HPF coefficient\n",
        "        OUTPUT: pre_emphasized signal array\n",
        "    \"\"\"\n",
        "    alpha=0.95\n",
        "    return lfilter([1., -alpha], 1, input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WjABucpexSe5"
      },
      "outputs": [],
      "source": [
        "def getmfcc(audiofile,sr,winSize,hopsize):\n",
        "    features=[]\n",
        "    for i in range(0,98):\n",
        "    #print(i)\n",
        "        windowedSig = audiofile[int((i*winSize)/2) : int((i*winSize)/2) + winSize]\n",
        "        mfcc = librosa.feature.mfcc(y=windowedSig,sr=16e3,n_mfcc=40)\n",
        "    #mfcc=np.mean(mfcc,axis=1)\n",
        "    #d_mfcc = librosa.feature.delta(mfcc)\n",
        "    #print(d_mfcc)\n",
        "    #dd_mfcc = librosa.feature.delta(d_mfcc,mode='constant')\n",
        "    #print(d_mfcc)\n",
        "        features.append(mfcc.reshape(1,40))\n",
        "    #features = np.concatenate([mfcc,d_mfcc,dd_mfcc],axis = 0)\n",
        "\n",
        "    #mfccs = librosa.feature.mfcc(y=windowedSig, sr=16000, n_mfcc=40)\n",
        "    return np.hstack(features)\n",
        "    #np.hstack(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk1KW5mkxSe6"
      },
      "outputs": [],
      "source": [
        "def load_data(folder):\n",
        "    os.chdir(folder)\n",
        "    data=[]\n",
        "    labels=[]\n",
        "    for f in os.listdir('./'):\n",
        "        print(f)\n",
        "        os.chdir(f)\n",
        "        for fil in os.listdir('./'):\n",
        "            #print(fil)\n",
        "            signal,sr = librosa.load(fil,sr=16000)\n",
        "            if(signal.shape[0]!=16000):\n",
        "                continue\n",
        "            #mf=getmfcc(signal,16000,320,160)\n",
        "            #reshaped_mf=mf.reshape(1,3920)\n",
        "            if(f=='down'):\n",
        "                label=[1,0,0,0,0,0,0,0,0,0]\n",
        "            elif(f=='go'):\n",
        "                label=[0,1,0,0,0,0,0,0,0,0]\n",
        "            elif(f=='left'):\n",
        "                label=[0,0,1,0,0,0,0,0,0,0]\n",
        "            elif(f=='no'):\n",
        "                label=[0,0,0,1,0,0,0,0,0,0]\n",
        "            elif(f=='off'):\n",
        "                label=[0,0,0,0,1,0,0,0,0,0]\n",
        "            elif(f=='on'):\n",
        "                label=[0,0,0,0,0,1,0,0,0,0]\n",
        "            elif(f=='right'):\n",
        "                label=[0,0,0,0,0,0,1,0,0,0]\n",
        "            elif(f=='stop'):\n",
        "                label=[0,0,0,0,0,0,0,1,0,0]\n",
        "            elif(f=='up'):\n",
        "                label=[0,0,0,0,0,0,0,0,1,0]\n",
        "            elif(f=='yes'):\n",
        "                label=[0,0,0,0,0,0,0,0,0,1]\n",
        "            data.append(signal)\n",
        "            labels.append(label)\n",
        "        os.chdir('../')    \n",
        "    os.chdir('../')\n",
        "    return data,labels\n",
        "                \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UpXrNbKDxSe7"
      },
      "outputs": [],
      "source": [
        "#train_data,train_labels=load_data('/Users/AJIT ZOTE/Desktop/Speech_Assignment_3/datasets/train')\n",
        "\n",
        "#np.save('train_data.npy', train_data, allow_pickle=True)\n",
        "\n",
        "#laoded_train_data = np.load('train_data.npy', allow_pickle=True)\n",
        "\n",
        "#save('train_labels.npy', train_labels, allow_pickle=True)\n",
        "\n",
        "#train_labels = np.load('train_labels.npy', allow_pickle=True)\n",
        "\n",
        "#features_train_data=[]\n",
        "#for i in laoded_train_data:\n",
        "#    m=getmfcc(i,16000,320,160)\n",
        "#    #print(\"s\")\n",
        "#    features_train_data.append(m)\n",
        "    \n",
        "\n",
        "#m=getmfcc(laoded_train_data[0],16000,320,160)\n",
        "#m.shape\n",
        "\n",
        "#np.save('features40_train_data.npy',features_train_data , allow_pickle=True)\n",
        "\n",
        "#features40_train_data = np.load('features40_train_data.npy', allow_pickle=True)\n",
        "\n",
        "#features40_train_data.shape[0]\n",
        "\n",
        "#features39_train_data=[]\n",
        "#count=1\n",
        "#for i in laoded_train_data:\n",
        "#    t=pre_emphasize(i)\n",
        "#    m=getmfcc(t,16000,320,160)\n",
        "#    print(count)\n",
        "#    count=count+1\n",
        "#    features39_train_data.append(m.reshape(1,39))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OB1yeAzxSe7",
        "outputId": "352930d0-4f2b-4103-ea46-31bfc38df3b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "down\n",
            "go\n",
            "left\n",
            "no\n",
            "off\n",
            "on\n",
            "right\n",
            "stop\n",
            "up\n",
            "yes\n"
          ]
        }
      ],
      "source": [
        "test_noisy_data,test_noisy_labels=load_data('/Users/AJIT ZOTE/Desktop/Speech_Assignment_3/datasets/test_noisy')\n",
        "\n",
        "np.save('test_noisy_data.npy', test_noisy_data, allow_pickle=True)\n",
        "np.save('test_noisy_labels.npy', test_noisy_labels, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsaUUxlCxSe8"
      },
      "outputs": [],
      "source": [
        "#np.save('features39_train_data.npy',features39_train_data , allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYU1OBqoxSe9"
      },
      "outputs": [],
      "source": [
        "train_labels = np.load('train_labels.npy', allow_pickle=True)\n",
        "train_labels=train_labels.astype('float64')\n",
        "targets=torch.from_numpy(train_labels)\n",
        "_,targets=torch.max(targets,dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C577rXGdxSe9"
      },
      "outputs": [],
      "source": [
        "test_noisy_data,test_noisy_labels=load_data('/Users/AJIT ZOTE/Desktop/Speech_Assignment_3/datasets/test_noisy')\n",
        "\n",
        "np.save('test_noisy_data.npy', test_noisy_data, allow_pickle=True)\n",
        "np.save('test_noisy_labels.npy', test_noisy_labels, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLYynF1sxSe-"
      },
      "outputs": [],
      "source": [
        "features_40_test_noisy_data=[]\n",
        "#count=1\n",
        "for i in test_noisy_data:\n",
        "    t=pre_emphasize(i)\n",
        "    m=getmfcc(t,16000,320,160)\n",
        "    #print(count)\n",
        "    #count=count+1\n",
        "    features_40_test_noisy_data.append(m.reshape(1,40*98))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OflEsuY6xSe_"
      },
      "outputs": [],
      "source": [
        "np.save('features40_test_noisy_data.npy',features_40_test_noisy_data , allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmlEc9oRxSe_"
      },
      "outputs": [],
      "source": [
        "test_clean_data = np.load('test_clean_data.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaQ_os1rxSe_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ3sXwKexSe_"
      },
      "outputs": [],
      "source": [
        "features_40_test_clean_data=[]\n",
        "#count=1\n",
        "for i in test_clean_data:\n",
        "    t=pre_emphasize(i)\n",
        "    m=getmfcc(t,16000,320,160)\n",
        "    #print(count)\n",
        "    #count=count+1\n",
        "    features_40_test_clean_data.append(m.reshape(1,40*98))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfe_VlsaxSfA"
      },
      "outputs": [],
      "source": [
        "np.save('features_40_test_clean_data.npy',features_40_test_clean_data , allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDWdx8PIxSfA"
      },
      "outputs": [],
      "source": [
        "train_data = np.load('train_data.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwE62xqDxSfA"
      },
      "outputs": [],
      "source": [
        "features_40_train_data_new=[]\n",
        "#count=1\n",
        "for i in train_data:\n",
        "    t=pre_emphasize(i)\n",
        "    m=getmfcc(t,16000,320,160)\n",
        "    #print(count)\n",
        "    #count=count+1\n",
        "    features_40_train_data_new.append(m.reshape(1,40*98))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeUCDsqfxSfB"
      },
      "outputs": [],
      "source": [
        "np.save('features_40_train_data_new.npy',features_40_train_data_new , allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S66Pj-kxSfB"
      },
      "outputs": [],
      "source": [
        "fu = np.load('features39_train_data.npy', allow_pickle=True)\n",
        "new_fu=fu.reshape(fu.shape[0],fu.shape[1]*fu.shape[2])\n",
        "inputs = torch.from_numpy(new_fu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srQ1Q_UQxSfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDTBxoLcxSfB"
      },
      "outputs": [],
      "source": [
        "print(targets.shape)\n",
        "print(inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoJs643PxSfB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "kCL2wxrOxSfC"
      },
      "outputs": [],
      "source": [
        "# Define dataset\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "#train_ds[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZinyyiDxSfC"
      },
      "outputs": [],
      "source": [
        "#train_ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aii6gq6LxSfC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [17065, 1896])\n",
        "len(train_ds), len(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvxyOwYixSfC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpvQAC1lxSfD"
      },
      "outputs": [],
      "source": [
        "input_size = 39\n",
        "num_classes = 10\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n",
        "model = model.double()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYgJdvVBxSfD"
      },
      "outputs": [],
      "source": [
        "#print(model.weight)\n",
        "#print(model.bias.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myZn5T5OxSfD"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_loader:\n",
        "    outputs = model(images)\n",
        "    break\n",
        "#print('outputs.shape : ', outputs.shape)\n",
        "#print('Sample outputs :\\n', outputs[:2].data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiHfLpB8xSfD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaQ4EgmvxSfD"
      },
      "outputs": [],
      "source": [
        "# Apply softmax for each output row\n",
        "probs = F.softmax(outputs, dim=1)\n",
        "# Look at sample probabilities\n",
        "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
        "\n",
        "# Add up the probabilities of an output row\n",
        "print(\"Sum: \", torch.sum(probs[0]).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2Lv79VDxSfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyl5IBXYxSfE"
      },
      "outputs": [],
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "#print(preds)\n",
        "#print(max_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tldqhRxgxSfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwDLhE6uxSfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsZpx20GxSfE"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    #_,labels=torch.max(labels,dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3w4vUpkOxSfE"
      },
      "outputs": [],
      "source": [
        "accuracy(outputs, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IIaxvRHxSfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8C0zPfNxSfF"
      },
      "outputs": [],
      "source": [
        "loss_fn = F.cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8-qxAG3xSfF"
      },
      "outputs": [],
      "source": [
        "\n",
        "loss = loss_fn(outputs, labels)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn_frh_LxSfF"
      },
      "outputs": [],
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
        "    \n",
        "model = MnistModel().double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDCykkTSxSfF"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dROi1wAJxSfF"
      },
      "outputs": [],
      "source": [
        "result0 = evaluate(model, val_loader)\n",
        "result0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA1NtQs3xSfF"
      },
      "outputs": [],
      "source": [
        "history1 = fit(5, 0.00001, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoR0kIvCxSfF"
      },
      "outputs": [],
      "source": [
        "history2 = fit(5, 0.00001, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "476OKZpuxSfG"
      },
      "outputs": [],
      "source": [
        "history3 = fit(5, 0.00001, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h01OEDUxSfG"
      },
      "outputs": [],
      "source": [
        "history4 = fit(5, 0.00001, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "g438v8UcxSfG"
      },
      "outputs": [],
      "source": [
        "# Replace these values with your results\n",
        "history = [result0] + history1 + history2 + history3 + history4\n",
        "accuracies = [result['val_acc'] for result in history]\n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1IUQdDjxSfG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-yuJQpHxSfG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN4mKs6kxSfG"
      },
      "outputs": [],
      "source": [
        "batch_size=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDa1ow2VxSfG"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjQ_pi3_xSfG"
      },
      "outputs": [],
      "source": [
        "class MnistModel(nn.Module):\n",
        "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
        "    def __init__(self, in_size, hidden_size, out_size):\n",
        "        super().__init__()\n",
        "        # hidden layer\n",
        "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
        "        # output layer\n",
        "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
        "        #self.linear3 = nn.Linear(hidden_size, out_size)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten the image tensors\n",
        "        #xb = xb.view(xb.size(0), -1)\n",
        "        # Get intermediate outputs using hidden layer\n",
        "        out = self.linear1(xb)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEf0pR9oxSfH"
      },
      "outputs": [],
      "source": [
        "input_size = 98*40\n",
        "hidden_size = 128 # you can change this\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu7KAxfHxSfH"
      },
      "outputs": [],
      "source": [
        "model = MnistModel(input_size, hidden_size=hidden_size, out_size=num_classes).double()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAaXLvl5xSfH"
      },
      "outputs": [],
      "source": [
        "for t in model.parameters():\n",
        "    print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB87sT0lxSfH"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_loader:\n",
        "    outputs = model(images)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    print('Loss:', loss.item())\n",
        "    break\n",
        "\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pg0RWlpxSfH"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me1qMKSYxSfH"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsJptaMXxSfI"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oTNkfpaxSfI"
      },
      "outputs": [],
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBDZ2JLixSfI"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    images = to_device(images, device)\n",
        "    print(images.device)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t10WovSlxSfJ"
      },
      "outputs": [],
      "source": [
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvquhS5dxSfJ"
      },
      "outputs": [],
      "source": [
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SDRwjITxSfJ"
      },
      "outputs": [],
      "source": [
        "for xb, yb in val_loader:\n",
        "    print('xb.device:', xb.device)\n",
        "    print('yb:', yb)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOpta_RlxSfJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwYushxDxSfJ"
      },
      "outputs": [],
      "source": [
        "# Model (on GPU)\n",
        "model = MnistModel(input_size, hidden_size=hidden_size, out_size=num_classes).double()\n",
        "to_device(model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YEwx6oOxSfJ"
      },
      "outputs": [],
      "source": [
        "history = [evaluate(model, val_loader)]\n",
        "history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJH8lkLuxSfK"
      },
      "outputs": [],
      "source": [
        "history += fit(25, 0.00001, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hMjyGKQHxSfK"
      },
      "outputs": [],
      "source": [
        "history += fit(25, 0.0001, model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "pPFkCx7oxSfK"
      },
      "outputs": [],
      "source": [
        "history += fit(5, 0.0001, model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl4RFOcXxSfK"
      },
      "outputs": [],
      "source": [
        "history += fit(5, 0.001, model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d98iyIOxSfK"
      },
      "outputs": [],
      "source": [
        "history += fit(5, 0.001, model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Maj34VYKxSfK"
      },
      "outputs": [],
      "source": [
        "history += fit(10, 0.0001, model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ4vGpfgxSfK"
      },
      "outputs": [],
      "source": [
        "history += fit(5, 0.001, model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUq6gPIvxSfL"
      },
      "outputs": [],
      "source": [
        "losses = [x['val_loss'] for x in history]\n",
        "plt.plot(losses, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_nwtku0xSfL"
      },
      "outputs": [],
      "source": [
        "accuracies = [x['val_acc'] for x in history]\n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQkHtYt5xSfL"
      },
      "outputs": [],
      "source": [
        "fu = np.load('features40_train_data.npy', allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyNJl3LjxSfL"
      },
      "outputs": [],
      "source": [
        "fu.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7j7_sAMxSfL"
      },
      "outputs": [],
      "source": [
        "#fu = np.load('features39_train_data.npy', allow_pickle=True)\n",
        "new_fu=fu.reshape(fu.shape[0],fu.shape[1]*fu.shape[2])\n",
        "inputs = torch.from_numpy(new_fu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0ShJGfHxSfL"
      },
      "outputs": [],
      "source": [
        "dataset = TensorDataset(inputs, targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr_g5LuKxSfL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [17065, 1896])\n",
        "len(train_ds), len(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EmGfE43xSfL"
      },
      "outputs": [],
      "source": [
        "batch_size=100\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSxdoKyWxSfM"
      },
      "outputs": [],
      "source": [
        "t = torch.cuda.get_device_properties(0).total_memory\n",
        "c = torch.cuda.memory_cached(0)\n",
        "a = torch.cuda.memory_allocated(0)\n",
        "f = c-a  # free inside cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib1fgNY3xSfM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-5mOknfxSfM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLAZ8xSzxSfM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htWRtzxJxSfM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3HlcfzjxSfM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}